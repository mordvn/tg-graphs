"""
Topic Analysis
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –æ–±—Å—É–∂–¥–µ–Ω–∏–π –≤ –≥—Ä—É–ø–ø–µ.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–≤ –∏ —Ñ—Ä–∞–∑.
"""
from collections import defaultdict, Counter
from datetime import datetime
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import re

# –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
STOP_WORDS = {
    # –†—É—Å—Å–∫–∏–µ
    '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∫', '–æ—Ç', '–∏–∑', '–∑–∞', '–æ', '–æ–±', '—É', '–¥–æ',
    '—á—Ç–æ', '–∫–∞–∫', '—ç—Ç–æ', '—Ç–∞–∫', '–Ω–æ', '–∞', '–∏–ª–∏', '–µ—Å–ª–∏', '—Ç–æ', '–∂–µ', '—Ç—ã', '—è',
    '–æ–Ω', '–æ–Ω–∞', '–º—ã', '–≤—ã', '–æ–Ω–∏', '–µ–≥–æ', '–µ—ë', '–∏—Ö', '–º–æ–π', '—Ç–≤–æ–π', '–Ω–∞—à', '–≤–∞—à',
    '–Ω–µ', '–¥–∞', '–Ω–µ—Ç', '–Ω—É', '–≤–æ—Ç', '–±—ã', '–ª–∏', '—É–∂–µ', '–µ—â—ë', '–µ—â–µ', '—Ç–æ–∂–µ', '–æ—á–µ–Ω—å',
    '—Ç–∞–º', '—Ç—É—Ç', '–∑–¥–µ—Å—å', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—Ç–æ–º', '—Å–µ–π—á–∞—Å', '–≤—Å—ë', '–≤—Å–µ', '—ç—Ç–æ—Ç',
    '—ç—Ç–∞', '—ç—Ç–∏', '—Ç–æ—Ç', '—Ç–∞', '—Ç–µ', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ', '—Ç–∞–∫–æ–π', '—Ç–∞–∫–∞—è',
    '–±—ã—Ç—å', '–µ—Å—Ç—å', '–±—ã–ª–æ', '–±—É–¥–µ—Ç', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–∏', '–º–æ–≥—É', '–º–æ–∂–Ω–æ', '–Ω–∞–¥–æ',
    '—Ç–æ–ª—å–∫–æ', '–ø—Ä–æ—Å—Ç–æ', '–¥–∞–∂–µ', '—á—Ç–æ–±—ã', '—Ö–æ—Ç—è', '—á–µ—Ä–µ–∑', '–ø–æ—Å–ª–µ', '–ø–µ—Ä–µ–¥', '–º–µ–∂–¥—É',
    # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ
    'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',
    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',
    'my', 'your', 'his', 'her', 'its', 'our', 'their', 'this', 'that', 'these', 'those',
    'what', 'which', 'who', 'whom', 'when', 'where', 'why', 'how',
    'and', 'or', 'but', 'if', 'then', 'so', 'than', 'too', 'very', 'just',
    'to', 'of', 'in', 'on', 'at', 'by', 'for', 'with', 'about', 'from',
    'not', 'no', 'yes', 'ok', 'okay',
}

# –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ–º
TOPIC_CATEGORIES = {
    'üíº –†–∞–±–æ—Ç–∞': ['—Ä–∞–±–æ—Ç–∞', '–æ—Ñ–∏—Å', '–Ω–∞—á–∞–ª—å–Ω–∏–∫', '–ø—Ä–æ–µ–∫—Ç', '–¥–µ–¥–ª–∞–π–Ω', '–º–∏—Ç–∏–Ω–≥', '–∑–∞–¥–∞—á–∞', '–∫–æ–ª–ª–µ–≥–∞'],
    'üéÆ –ò–≥—Ä—ã': ['–∏–≥—Ä–∞', '–∏–≥—Ä–∞—Ç—å', 'steam', 'ps5', 'xbox', '–≥–µ–π–º–µ—Ä', '–º–∞—Ç—á', '—Ä–µ–π–¥', '–≥–∏–ª—å–¥–∏—è'],
    'üé¨ –ö–∏–Ω–æ/–°–µ—Ä–∏–∞–ª—ã': ['—Ñ–∏–ª—å–º', '—Å–µ—Ä–∏–∞–ª', 'netflix', '–∫–∏–Ω–æ', '—Å–º–æ—Ç—Ä–µ—Ç—å', '–∞–∫—Ç—ë—Ä', '—Å–µ—Ä–∏—è', '—Å–µ–∑–æ–Ω'],
    'üçï –ï–¥–∞': ['–µ–¥–∞', '—Ä–µ—Å—Ç–æ—Ä–∞–Ω', '–∫–∞—Ñ–µ', '–ø–∏—Ü—Ü–∞', '—Å—É—à–∏', '–±—É—Ä–≥–µ—Ä', '–≥–æ—Ç–æ–≤–∏—Ç—å', '–≤–∫—É—Å–Ω–æ', '–¥–æ—Å—Ç–∞–≤–∫–∞'],
    'üèãÔ∏è –°–ø–æ—Ä—Ç/–ó–¥–æ—Ä–æ–≤—å–µ': ['—Å–ø–æ—Ä—Ç', '—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞', '–∑–∞–ª', '–±–µ–≥', '—Ñ—É—Ç–±–æ–ª', '–∑–¥–æ—Ä–æ–≤—å–µ', '–≤—Ä–∞—á', '–±–æ–ª–µ—Ç—å'],
    'üéµ –ú—É–∑—ã–∫–∞': ['–º—É–∑—ã–∫–∞', '–ø–µ—Å–Ω—è', '–∫–æ–Ω—Ü–µ—Ä—Ç', '–∞–ª—å–±–æ–º', 'spotify', '—Å–ª—É—à–∞—Ç—å', '—Ç—Ä–µ–∫', '–≥—Ä—É–ø–ø–∞'],
    'üì± –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': ['—Ç–µ–ª–µ—Ñ–æ–Ω', 'apple', 'android', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ', '–±–∞–≥', '–∫–æ–¥', '–ø—Ä–æ–≥—Ä–∞–º–º–∞'],
    '‚úàÔ∏è –ü—É—Ç–µ—à–µ—Å—Ç–≤–∏—è': ['–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ', '–æ—Ç–ø—É—Å–∫', '–±–∏–ª–µ—Ç', '–æ—Ç–µ–ª—å', '–ø–æ–µ–∑–¥–∫–∞', '—Å—Ç—Ä–∞–Ω–∞', '–≥–æ—Ä–æ–¥', '–≤–∏–∑–∞'],
    'üí∞ –î–µ–Ω—å–≥–∏': ['–¥–µ–Ω—å–≥–∏', '–∑–∞—Ä–ø–ª–∞—Ç–∞', '–∫—Ä–µ–¥–∏—Ç', '–±–∞–Ω–∫', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', '–∫—Ä–∏–ø—Ç–∞', '–∫—É—Ä—Å', '–¥–æ—Ä–æ–≥–æ'],
    '‚ù§Ô∏è –û—Ç–Ω–æ—à–µ–Ω–∏—è': ['–¥–µ–≤—É—à–∫–∞', '–ø–∞—Ä–µ–Ω—å', '—Å–≤–∏–¥–∞–Ω–∏–µ', '–æ—Ç–Ω–æ—à–µ–Ω–∏—è', '–ª—é–±–æ–≤—å', '—Å–≤–∞–¥—å–±–∞', '—Ä–∞—Å—Å—Ç–∞–ª–∏—Å—å'],
}


def get_text(msg):
    text = msg.get('text', '')
    if isinstance(text, list):
        parts = []
        for part in text:
            if isinstance(part, str):
                parts.append(part)
            elif isinstance(part, dict) and 'text' in part:
                parts.append(part['text'])
        return ' '.join(parts)
    return str(text) if text else ''


def parse_date(date_str):
    return datetime.strptime(date_str, "%Y-%m-%dT%H:%M:%S")


def extract_words(text):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã
    text = re.sub(r'[^\w\s]', ' ', text.lower())
    words = text.split()
    # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    return [w for w in words if len(w) > 2 and w not in STOP_WORDS]


def run_plugin(data):
    messages = data.get("messages", [])
    chat_name = data.get("name", "Chat")
    
    if not messages:
        st.warning("–ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
        return
    
    st.subheader(f"üí¨ –ê–Ω–∞–ª–∏–∑ –¢–µ–º ‚Äî {chat_name}")
    st.markdown("–û —á—ë–º —á–∞—â–µ –≤—Å–µ–≥–æ –≥–æ–≤–æ—Ä—è—Ç –≤ –≥—Ä—É–ø–ø–µ")
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞
    all_words = []
    user_words = defaultdict(list)
    monthly_words = defaultdict(list)
    
    for msg in messages:
        sender = msg.get('from')
        if not sender:
            continue
        
        text = get_text(msg)
        words = extract_words(text)
        
        all_words.extend(words)
        user_words[sender].extend(words)
        
        try:
            dt = parse_date(msg['date'])
            month = dt.strftime('%Y-%m')
            monthly_words[month].extend(words)
        except:
            pass
    
    if not all_words:
        st.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
        return
    
    # –ß–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤
    word_freq = Counter(all_words)
    
    # –¢–æ–ø —Å–ª–æ–≤
    st.markdown("### üî§ –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞")
    
    top_words = word_freq.most_common(30)
    
    col1, col2 = st.columns(2)
    
    with col1:
        # –¢–∞–±–ª–∏—Ü–∞
        df_words = pd.DataFrame(top_words, columns=['–°–ª–æ–≤–æ', '–ß–∞—Å—Ç–æ—Ç–∞'])
        st.dataframe(df_words, hide_index=True)
    
    with col2:
        # –û–±–ª–∞–∫–æ —Å–ª–æ–≤ (bar chart)
        fig1, ax1 = plt.subplots(figsize=(8, 8))
        
        words_20 = top_words[:20]
        words_list = [w[0] for w in words_20]
        counts_list = [w[1] for w in words_20]
        
        ax1.barh(words_list[::-1], counts_list[::-1], color='steelblue')
        ax1.set_xlabel('–ß–∞—Å—Ç–æ—Ç–∞')
        ax1.set_title('–¢–æ–ø-20 —Å–ª–æ–≤')
        
        plt.tight_layout()
        st.pyplot(fig1)
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Ç–µ–º
    st.markdown("### üìä –¢–µ–º—ã –æ–±—Å—É–∂–¥–µ–Ω–∏–π")
    
    topic_counts = {}
    for topic, keywords in TOPIC_CATEGORIES.items():
        count = sum(word_freq.get(kw, 0) for kw in keywords)
        if count > 0:
            topic_counts[topic] = count
    
    if topic_counts:
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º
        sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)
        
        fig2, ax2 = plt.subplots(figsize=(10, 6))
        
        topics = [t[0] for t in sorted_topics]
        counts = [t[1] for t in sorted_topics]
        
        bars = ax2.bar(topics, counts, color='coral')
        ax2.set_ylabel('–£–ø–æ–º–∏–Ω–∞–Ω–∏–π')
        ax2.set_title('–ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–µ–º')
        ax2.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        st.pyplot(fig2)
        
        # –û–ø–∏—Å–∞–Ω–∏–µ
        st.markdown("**–¢–æ–ø-3 —Ç–µ–º—ã:**")
        for topic, count in sorted_topics[:3]:
            keywords = TOPIC_CATEGORIES[topic]
            found_keywords = [(kw, word_freq.get(kw, 0)) for kw in keywords if word_freq.get(kw, 0) > 0]
            found_keywords.sort(key=lambda x: x[1], reverse=True)
            kw_str = ', '.join(f"{kw} ({c})" for kw, c in found_keywords[:5])
            st.write(f"**{topic}**: {count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π ‚Äî {kw_str}")
    else:
        st.info("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ–º—ã")
    
    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –ø–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º
    st.markdown("### üë§ –•–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤")
    
    users = list(user_words.keys())
    
    # –ù–∞—Ö–æ–¥–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_unique = {}
    for user in users:
        user_freq = Counter(user_words[user])
        
        # –í—ã—á–∏—Å–ª—è–µ–º TF-IDF-like –º–µ—Ç—Ä–∏–∫—É
        unique_words = []
        for word, count in user_freq.most_common(50):
            # –°–∫–æ–ª—å–∫–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç —ç—Ç–æ —Å–ª–æ–≤–æ
            users_with_word = sum(1 for u in users if word in user_words[u])
            # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å = —á–∞—Å—Ç–æ—Ç–∞ * (1 / –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å–æ —Å–ª–æ–≤–æ–º)
            uniqueness = count * (len(users) / users_with_word)
            unique_words.append((word, uniqueness, count))
        
        unique_words.sort(key=lambda x: x[1], reverse=True)
        user_unique[user] = unique_words[:10]
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
    for user in sorted(users, key=lambda u: len(user_words[u]), reverse=True)[:10]:
        if user_unique[user]:
            words_str = ', '.join(f"**{w[0]}** ({w[2]})" for w in user_unique[user][:5])
            st.write(f"üë§ **{user}**: {words_str}")
    
    # –î–∏–Ω–∞–º–∏–∫–∞ —Ç–µ–º –ø–æ –º–µ—Å—è—Ü–∞–º
    if len(monthly_words) > 3:
        st.markdown("### üìà –î–∏–Ω–∞–º–∏–∫–∞ —Ç–µ–º –ø–æ –º–µ—Å—è—Ü–∞–º")
        
        months = sorted(monthly_words.keys())
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-5 —Ç–µ–º –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
        top_topics = sorted_topics[:5] if topic_counts else []
        
        if top_topics:
            fig3, ax3 = plt.subplots(figsize=(12, 5))
            
            for topic, _ in top_topics:
                keywords = TOPIC_CATEGORIES[topic]
                values = []
                for month in months:
                    month_freq = Counter(monthly_words[month])
                    count = sum(month_freq.get(kw, 0) for kw in keywords)
                    values.append(count)
                
                ax3.plot(months, values, marker='o', label=topic, linewidth=2)
            
            ax3.set_xlabel('–ú–µ—Å—è—Ü')
            ax3.set_ylabel('–£–ø–æ–º–∏–Ω–∞–Ω–∏–π')
            ax3.set_title('–ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–µ–º –ø–æ –º–µ—Å—è—Ü–∞–º')
            ax3.legend()
            ax3.tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            st.pyplot(fig3)
    
    # –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ñ–∞–∫—Ç—ã
    st.markdown("### üí° –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ñ–∞–∫—Ç—ã")
    
    # –°–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ —á–∞—Å—Ç–æ–µ —Å–ª–æ–≤–æ
    long_words = [(w, c) for w, c in word_freq.items() if len(w) > 8 and c > 5]
    if long_words:
        longest = max(long_words, key=lambda x: len(x[0]))
        st.info(f"üìù –°–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ–µ —Å–ª–æ–≤–æ: **{longest[0]}** ({longest[1]} —Ä–∞–∑)")
    
    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –≥—Ä—É–ø–ø—ã
    unique_vocab = len(word_freq)
    total_words = sum(word_freq.values())
    st.info(f"üìö –°–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å –≥—Ä—É–ø–ø—ã: **{unique_vocab}** —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ {total_words} –æ–±—â–∏—Ö")
    
    # –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ
    avg_words = total_words / len(messages) if messages else 0
    st.info(f"üí¨ –°—Ä–µ–¥–Ω–µ–µ —Å–ª–æ–≤ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ: **{avg_words:.1f}**")

